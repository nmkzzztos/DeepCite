<!doctype html>
<html lang="en-GB" dir="ltr" class="plugin-pages plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.1">
<title data-rh="true">Papers - DeepCite | Mind Notes</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://github.com/DeepCite/img/logo.svg"><meta data-rh="true" name="twitter:image" content="https://github.com/DeepCite/img/logo.svg"><meta data-rh="true" property="og:url" content="https://github.com/DeepCite/papers"><meta data-rh="true" property="og:locale" content="en_GB"><meta data-rh="true" property="og:locale:alternate" content="de_DE"><meta data-rh="true" property="og:locale:alternate" content="ru_RU"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Papers - DeepCite | Mind Notes"><meta data-rh="true" name="description" content="Curated collection of influential scientific papers across AI, quantum computing, cryptography, and NLP"><meta data-rh="true" property="og:description" content="Curated collection of influential scientific papers across AI, quantum computing, cryptography, and NLP"><link data-rh="true" rel="icon" href="/DeepCite/img/logo_mini.svg"><link data-rh="true" rel="canonical" href="https://github.com/DeepCite/papers"><link data-rh="true" rel="alternate" href="https://github.com/DeepCite/papers" hreflang="en-GB"><link data-rh="true" rel="alternate" href="https://github.com/DeepCite/de/papers" hreflang="de-DE"><link data-rh="true" rel="alternate" href="https://github.com/DeepCite/ru/papers" hreflang="ru-RU"><link data-rh="true" rel="alternate" href="https://github.com/DeepCite/papers" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/DeepCite/blog/rss.xml" title="Mind Notes RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/DeepCite/blog/atom.xml" title="Mind Notes Atom Feed">





<script src="/js/math-copy.js"></script><link rel="stylesheet" href="/DeepCite/assets/css/styles.51925d8a.css">
<script src="/DeepCite/assets/js/runtime~main.b3c70652.js" defer="defer"></script>
<script src="/DeepCite/assets/js/main.f1cd8191.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/DeepCite/"><div class="navbar__logo"><img src="/DeepCite/img/logo_mini.svg" alt="Mind Notes" class="header-logo themedComponent_mlkZ themedComponent--light_NVdE"><img src="/DeepCite/img/logo_mini_white.svg" alt="Mind Notes" class="header-logo themedComponent_mlkZ themedComponent--dark_xIcU"></div></a><a class="navbar__item navbar__link" href="/DeepCite/docs/introduction">Docs</a><a class="navbar__item navbar__link" href="/DeepCite/papers">Papers</a><a class="navbar__item navbar__link" href="/DeepCite/projects">Projects</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/nmkzzztos/DeepCite" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link header-github-link header-gitlab-link" aria-label="GitHub repository"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z" dir="ltr"><input placeholder="Search" aria-label="Search" class="navbar__search-input searchInput_YFbd" value=""><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="container_Kx51"><header class="header_AFJS"><h1 class="title_Y2nq">Papers</h1><p class="subtitle_yMe8">Curated collection of influential research papers across LLM, RAG, Agentic AI, etc.</p></header><main><section class="categorySection_CdSb"><h2 class="categoryTitle_XkS0">RAG (Retrieval Augmented Generation)</h2><div class="papersGrid_uBqp"><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2508.21038v1" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">On the Theoretical Limitations of Embedding-Based Retrieval</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Orion Weller, Michael Boratko, Iftekhar Naim, Jinhyuk Lee</span><div><span class="year_plco">(<!-- -->2025<!-- -->) </span><span class="citationsCount_r4Vh">15<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2508.21038v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2508.21038v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2501.09136" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Aditi Singh, Abul Ehtesham, Saket Kumar, Tala Talaei Khoei</span><div><span class="year_plco">(<!-- -->2025<!-- -->) </span><span class="citationsCount_r4Vh">111<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2501.09136" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="https://arxiv.org/pdf/2501.09136.pdf" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2402.02716v1" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Understanding the planning of LLM agents: A survey</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, Enhong Chen</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="citationsCount_r4Vh">304<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2402.02716v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2402.02716v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2402.19473" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Retrieval-Augmented Generation for AI-Generated Content: A Survey</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, Bin Cui</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="citationsCount_r4Vh">520<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2402.19473" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="https://arxiv.org/pdf/2402.19473.pdf" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2407.01219" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Searching for Best Practices in Retrieval-Augmented Generation</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, Ruicheng Yin, Changze Lv, Xiaoqing Zheng, Xuanjing Huang</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="citationsCount_r4Vh">168<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2407.01219" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="https://arxiv.org/pdf/2407.01219.pdf" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2305.11738v4" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">CRITIC: Large Language Models Can Self-Correct with Tool-Interactive
  Critiquing</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">485<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2305.11738v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2305.11738v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2303.11366v4" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Reflexion: Language Agents with Verbal Reinforcement Learning</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">2,431<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2303.11366v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2303.11366v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2303.17651v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Self-Refine: Iterative Refinement with Self-Feedback</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, Peter Clark</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">2,444<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2303.17651v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2303.17651v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2305.15294v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Enhancing Retrieval-Augmented Large Language Models with Iterative
  Retrieval-Generation Synergy</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, Weizhu Chen</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">381<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2305.15294v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2305.15294v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2312.10997" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Retrieval-Augmented Generation for Large Language Models: A Survey</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">3,343<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2312.10997" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="https://arxiv.org/pdf/2312.10997.pdf" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2305.14283" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Query Rewriting for Retrieval-Augmented Large Language Models</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">495<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2305.14283" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="https://arxiv.org/pdf/2305.14283.pdf" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2212.14024v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Demonstrate-Search-Predict: Composing retrieval and language models for
  knowledge-intensive NLP</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts, Matei Zaharia</span><div><span class="year_plco">(<!-- -->2022<!-- -->) </span><span class="citationsCount_r4Vh">325<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2212.14024v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2212.14024v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2212.10496" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Precise Zero-Shot Dense Retrieval without Relevance Labels</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Luyu Gao, Xueguang Ma, Jimmy Lin, Jamie Callan</span><div><span class="year_plco">(<!-- -->2022<!-- -->) </span><span class="citationsCount_r4Vh">499<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2212.10496" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="https://arxiv.org/pdf/2212.10496.pdf" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2005.11401v4" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela</span><div><span class="year_plco">(<!-- -->2020<!-- -->) </span><span class="citationsCount_r4Vh">12,239<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2005.11401v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2005.11401v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div></div></section><section class="categorySection_CdSb"><h2 class="categoryTitle_XkS0">Parsing</h2><div class="papersGrid_uBqp"><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2501.17887v1" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Docling: An Efficient Open-Source Toolkit for AI-driven Document  Conversion</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Nikolaos Livathinos, Christoph Auer, Maksym Lysak, Ahmed Nassar, Michele Dolfi, Panos Vagenas, Cesar Berrospi Ramis, Matteo Omenetti, Kasper Dinkla, Yusik Kim, Shubham Gupta, Rafael Teixeira de Lima, Valery Weber, Lucas Morin, Ingmar Meijer, Viktor Kuropiatnyk, Peter W. J. Staar</span><div><span class="year_plco">(<!-- -->2025<!-- -->) </span><span class="citationsCount_r4Vh">16<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2501.17887v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2501.17887v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2410.09871v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">A Comparative Study of PDF Parsing Tools Across Diverse Document Categories</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Narayan S. Adhikari, Shradha Agarwal</span><div><span class="year_plco">(<!-- -->2025<!-- -->) </span><span class="citationsCount_r4Vh">19<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2410.09871v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2410.09871v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2303.09957v1" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">A Benchmark of PDF Information Extraction Tools using a Multi-Task and Multi-Domain Evaluation Framework for Academic Documents</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Norman Meuschke, Apurva Jagdale, Timo Spinde, Jelena Mitrović, Bela Gipp</span><div><span class="year_plco">(<!-- -->2025<!-- -->) </span><span class="citationsCount_r4Vh">33<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2303.09957v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2303.09957v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2412.07626v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">OmniDocBench: Benchmarking Diverse PDF Document Parsing with  Comprehensive Annotations</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, Jin Shi, Fan Wu, Pei Chu, Minghao Liu, Zhenxiang Li, Chao Xu, Bo Zhang, Botian Shi, Zhongying Tu, Conghui He</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="citationsCount_r4Vh">14<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2412.07626v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2412.07626v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div></div></section><section class="categorySection_CdSb"><h2 class="categoryTitle_XkS0">LLM</h2><div class="papersGrid_uBqp"><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2311.05232v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">A Survey on Hallucination in Large Language Models: Principles,
  Taxonomy, Challenges, and Open Questions</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="citationsCount_r4Vh">700<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2311.05232v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2311.05232v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2402.16775v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">A Comprehensive Evaluation of Quantization Strategies for Large Language Models</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="citationsCount_r4Vh">51<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2402.16775v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2402.16775v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2402.06196v3" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Large Language Models: A Survey</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, Jianfeng Gao</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="citationsCount_r4Vh">1,432<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2402.06196v3" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2402.06196v3" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2304.00685v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Vision-Language Models for Vision Tasks: A Survey</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Jingyi Zhang, Jiaxing Huang, Sheng Jin, Shijian Lu</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">1,035<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2304.00685v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2304.00685v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2401.11817v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Hallucination is Inevitable: An Innate Limitation of Large Language
  Models</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Ziwei Xu, Sanjay Jain, Mohan Kankanhalli</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">2,913<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2401.11817v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2401.11817v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2302.13971v1" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">LLaMA: Open and Efficient Foundation Language Models</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">20,564<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2302.13971v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2302.13971v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2305.17473v4" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Farhad Mortezapour Shiri, Thinagaran Perumal, Norwati Mustapha, Raihani Mohamed</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">445<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2305.17473v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2305.17473v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2204.02311v5" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">PaLM: Scaling Language Modeling with Pathways</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, Noah Fiedel</span><div><span class="year_plco">(<!-- -->2022<!-- -->) </span><span class="citationsCount_r4Vh">7,624<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2204.02311v5" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2204.02311v5" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2005.14165v4" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Language Models are Few-Shot Learners</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, Dario Amodei</span><div><span class="year_plco">(<!-- -->2020<!-- -->) </span><span class="citationsCount_r4Vh">56,058<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2005.14165v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2005.14165v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/1810.04805v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">BERT: Pre-training of Deep Bidirectional Transformers for Language  Understanding</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova</span><div><span class="year_plco">(<!-- -->2018<!-- -->) </span><span class="citationsCount_r4Vh">146,523<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/1810.04805v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/1810.04805v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/1706.03762v7" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Attention Is All You Need</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</span><div><span class="year_plco">(<!-- -->2017<!-- -->) </span><span class="citationsCount_r4Vh">198,836<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/1706.03762v7" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/1706.03762v7" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://dl.acm.org/doi/10.5555/176313.176316" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Class-based n-gram models of natural language</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Brown, Peter F. and deSouza, Peter V. and Mercer, Robert L. and Pietra, Vincent J. Della and Lai, Jenifer C.</span><div><span class="year_plco">(<!-- -->1994<!-- -->) </span><span class="citationsCount_r4Vh">4,805<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://dl.acm.org/doi/pdf/10.5555/176313.176316" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div></div></section><section class="categorySection_CdSb"><h2 class="categoryTitle_XkS0">Prompting</h2><div class="papersGrid_uBqp"><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2402.07927v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, Aman Chadha</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="citationsCount_r4Vh">1,050<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2402.07927v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2402.07927v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2305.10601v2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">4,294<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2305.10601v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2305.10601v2" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2308.09687v4" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Graph of Thoughts: Solving Elaborate Problems with Large Language Models</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, Torsten Hoefler</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">1,325<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2308.09687v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2308.09687v4" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2311.08734v1" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Thread of Thought Unraveling Chaotic Contexts</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Yucheng Zhou, Xiubo Geng, Tao Shen, Chongyang Tao, Guodong Long, Jian-Guang Lou, Jianbing Shen</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">87<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2311.08734v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2311.08734v1" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2201.11903v6" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, Denny Zhou</span><div><span class="year_plco">(<!-- -->2022<!-- -->) </span><span class="citationsCount_r4Vh">20,506<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2201.11903v6" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="http://arxiv.org/pdf/2201.11903v6" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div></div></section><section class="categorySection_CdSb"><h2 class="categoryTitle_XkS0">Agentic AI</h2><div class="papersGrid_uBqp"><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2401.03428" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Yuheng Cheng, Ceyao Zhang, Zhengwen Zhang, Xiangrui Meng, Sirui Hong, Wenhao Li, Zihao Wang, Zekai Wang, Feng Yin, Junhua Zhao, Xiuqiang He</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="citationsCount_r4Vh">163<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2401.03428" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="https://arxiv.org/pdf/2401.03428.pdf" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://link.springer.com/article/10.1007/s44336-024-00009-2" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, Yi Yang</span><div><span class="year_plco">(<!-- -->2024<!-- -->) </span><span class="journal_kLOW">Machine Intelligence Research</span><span class="citationsCount_r4Vh">199<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://doi.org/10.1007/s44336-024-00009-2" target="_blank" rel="noopener noreferrer" class="link_s7rq">DOI</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div><div class="paperCard_ReeH"><div class="paperContent_je0v"><div class="paperMain_vcy9"><h3 class="paperTitle_WTB3"><a href="https://arxiv.org/abs/2309.07864" target="_blank" rel="noopener noreferrer" class="paperLink_LeDX">The Rise and Potential of Large Language Model Based Agents: A Survey</a></h3><div class="paperMeta_DZyK"><span class="authors_t6mq">Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, Tao Gui</span><div><span class="year_plco">(<!-- -->2023<!-- -->) </span><span class="citationsCount_r4Vh">1,501<!-- --> citations</span></div></div></div><div class="paperLinks_mXzB"><a href="https://arxiv.org/abs/2309.07864" target="_blank" rel="noopener noreferrer" class="link_s7rq">arXiv</a><a href="https://arxiv.org/pdf/2309.07864.pdf" target="_blank" rel="noopener noreferrer" class="link_s7rq">PDF</a><button class="citeButton_qFQ8">Cite</button><button class="toggleButton_gNiU">Show abstract</button></div></div></div></div></section></main></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title"></div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/nmkzzztos" target="_blank" rel="noopener noreferrer" class="footer__link-item footer__link-github">Github<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title"></div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.linkedin.com/in/nmkzzztos/" target="_blank" rel="noopener noreferrer" class="footer__link-item footer__link-linkedin">LinkedIn<svg width="13.5" height="13.5" aria-label="(opens in new tab)" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">© 2025 Anton Guliaev | Powered by <a href="https://docusaurus.io/" target="_blank">Docusaurus</a></div></div></div></footer></div>
</body>
</html>